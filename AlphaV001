import re
class OpenTokenize:

    def OpenText(self):

        for element in files:
            with open(element,'r') as fin:
                #filename = fin.name
                #filename = filename.rsplit('/')[-1]
                self.text = fin.read()

    def clean_text(self):

        self.cleaned = re.sub(r'{.+}{.+}|\|',' ',self.text)
        self.cleaned = re.sub(r'"','', self.cleaned)
        #self.removed = re.sub(r'\|','',self.removed)

    def make_lower(self):

        self.lowered_text=[y.lower() for y in self.cleaned]

    def convert_list_to_str(self):

        self.string = ''.join(self.lowered_text)
        #print(self.string)
    def tokenize(self):

       
        self.tokenized = re.findall(r'\b[a-z]+\.[a-z]+\.[a-z]+|(?:\d+\.\d+)|(?:\d+\,\d+)|(?:\d+\:\d+)|[a-z]+\'[a-z]+|[a-z]+\'|\'[a-z]+|\w+(?:-\w+)*|\S\b',
                                    self.string) #Tokenize
        self.tokenized = ' '.join(self.tokenized) #Convert back to string
        self.tokenized = re.sub(r' ','\n', self.tokenized) #Replace space with newline
        
        print(self.tokenized)
class Node(object):
    
    def __init__(self, data):
        self.data = data
        self.next = None

class LinkedList():
    def __init__(self):
        self.head = None
        self.tail = None

    def AddNode(self,data):
        new_node = Node(data)
        
        if self.head == None:
            self.head = new_node
        if self.tail != None:
            self.tail.next = new_node
        self.tail = new_node
    
    def RemoveNode(self, index):
        prev = Node
        node = self.head
        i = 0

        while (node != None) and (i < index):
            prev = node
            node = node.next
            i += 1
        if prev == None:
            self.head = node.next
        else:
            prev.next = node.next

    def PrintList(self):
        node = self.head
        while node != None:
            print(node.data)
            node = node.next

files = (['/home/stp14/filtorn/programmering_2/projekt/Pulle.sub'])
read = OpenTokenize()

read.OpenText()
read.clean_text()
read.make_lower()
read.convert_list_to_str()
read.tokenize()
